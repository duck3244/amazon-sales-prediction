# Amazon E-Commerce Sales Prediction Project

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”
ì´ í”„ë¡œì íŠ¸ëŠ” Amazon íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ íŒë§¤ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- ğŸ” **í¬ê´„ì ì¸ ë°ì´í„° ë¶„ì„**: ì‹œê°í™” ë° í†µê³„ ë¶„ì„
- ğŸ› ï¸ **ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**: ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŠ¹ì„± ì¶”ì¶œ, ì •ê·œí™”
- ğŸ¤– **3ê°€ì§€ ëª¨ë¸ ì•„í‚¤í…ì²˜**: Basic, Advanced (Residual), Attention
- ğŸ“Š **ê³ ê¸‰ í•™ìŠµ ê¸°ëŠ¥**: Early Stopping, Learning Rate Scheduling, Dropout
- ğŸ“ˆ **ì¢…í•©ì ì¸ í‰ê°€**: MAE, RMSE, RÂ² ë“± ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œ
- ğŸ¨ **í’ë¶€í•œ ì‹œê°í™”**: í•™ìŠµ ê³¡ì„ , ì˜ˆì¸¡ ê²°ê³¼, ìƒê´€ê´€ê³„ ë¶„ì„

## ğŸ“¦ ë°ì´í„°ì…‹
- **ì¶œì²˜**: [Kaggle - E-Commerce Sales Dataset](https://www.kaggle.com/datasets/thedevastator/unlock-profits-with-e-commerce-sales-data)
- **íŒŒì¼**: Amazon Sale Report.csv

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°
```
amazon-sales-prediction/
â”‚
â”œâ”€â”€ README.md                          # í”„ë¡œì íŠ¸ ë¬¸ì„œ
â”œâ”€â”€ requirements.txt                   # Python íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ run_guide.sh                       # ì‹¤í–‰ ê°€ì´ë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ amazon_sales_prediction.ipynb      # Jupyter ë…¸íŠ¸ë¶
â”‚
â”œâ”€â”€ ğŸ“Š ë°ì´í„° íŒŒì¼
â”‚   â””â”€â”€ Amazon Sale Report.csv         # Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ í•„ìš”
â”‚
â”œâ”€â”€ ğŸ”§ í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ data_analysis.py               # ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
â”‚   â”œâ”€â”€ data_preprocessing.py          # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ model.py                       # PyTorch ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ train.py                       # ëª¨ë¸ í•™ìŠµ
â”‚   â”œâ”€â”€ predict.py                     # ì˜ˆì¸¡ ë° í‰ê°€
â”‚   â””â”€â”€ utils.py                       # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚
â”œâ”€â”€ ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° (ìë™ ìƒì„±)
â”‚   â”œâ”€â”€ X_train.npy, y_train.npy      # í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ X_val.npy, y_val.npy          # ê²€ì¦ ë°ì´í„°
â”‚   â”œâ”€â”€ X_test.npy, y_test.npy        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â””â”€â”€ preprocessor.pkl               # ì „ì²˜ë¦¬ ê°ì²´
â”‚
â”œâ”€â”€ ğŸ¤– ëª¨ë¸ ë””ë ‰í† ë¦¬ (ìë™ ìƒì„±)
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ best_model.pth             # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”‚       â”œâ”€â”€ config.json                # ëª¨ë¸ ì„¤ì •
â”‚       â””â”€â”€ training_history.png       # í•™ìŠµ ê³¡ì„ 
â”‚
â””â”€â”€ ğŸ“ˆ ê²°ê³¼ ë””ë ‰í† ë¦¬ (ìë™ ìƒì„±)
    â””â”€â”€ results/
        â”œâ”€â”€ evaluation_results.json    # ì„±ëŠ¥ ì§€í‘œ
        â”œâ”€â”€ predictions.csv            # ì˜ˆì¸¡ ê²°ê³¼
        â””â”€â”€ prediction_visualization.png
```

---

## ğŸš€ ì‹œì‘í•˜ê¸°

### Step 1: í™˜ê²½ ì„¤ì •

#### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Python 3.8 ì´ìƒ
- pip íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €

#### íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

**ì„¤ì¹˜ë˜ëŠ” ì£¼ìš” íŒ¨í‚¤ì§€:**
- `torch>=2.0.0` - PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- `numpy>=1.24.0` - ìˆ˜ì¹˜ ê³„ì‚°
- `pandas>=2.0.0` - ë°ì´í„° ì²˜ë¦¬
- `scikit-learn>=1.3.0` - ì „ì²˜ë¦¬ ë° í‰ê°€
- `matplotlib>=3.7.0` - ì‹œê°í™”
- `seaborn>=0.12.0` - ê³ ê¸‰ ì‹œê°í™”

---

### Step 2: ë°ì´í„° ë‹¤ìš´ë¡œë“œ

1. Kaggle ì›¹ì‚¬ì´íŠ¸ ì ‘ì†
   ```
   https://www.kaggle.com/datasets/thedevastator/unlock-profits-with-e-commerce-sales-data
   ```

2. `Amazon Sale Report.csv` íŒŒì¼ ë‹¤ìš´ë¡œë“œ

3. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ì €ì¥
   ```
   amazon-sales-prediction/
   â””â”€â”€ Amazon Sale Report.csv  â† ì—¬ê¸°ì— ì €ì¥
   ```

---

### Step 3: ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„

#### ì‹¤í–‰ ëª…ë ¹ì–´
```bash
python data_analysis.py
```

#### ìˆ˜í–‰ ì‘ì—…
- âœ… ë°ì´í„° ê¸°ë³¸ ì •ë³´ ì¶œë ¥ (shape, ì»¬ëŸ¼, ë°ì´í„° íƒ€ì…)
- âœ… ê¸°ìˆ  í†µê³„ ê³„ì‚°
- âœ… ê²°ì¸¡ì¹˜ ë¶„ì„
- âœ… ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ì‹œê°í™”
- âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„
- âœ… ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„±
- âœ… ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)

#### ìƒì„±ë˜ëŠ” íŒŒì¼
```
ğŸ“Š numerical_features_distribution.png  # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬
ğŸ“Š correlation_heatmap.png              # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
ğŸ“Š [ë³€ìˆ˜ëª…]_distribution.png            # ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ë“¤
ğŸ“„ data_summary.txt                     # ë°ì´í„° ìš”ì•½ ì •ë³´
```

---

### Step 4: ë°ì´í„° ì „ì²˜ë¦¬

#### ì‹¤í–‰ ëª…ë ¹ì–´
```bash
python data_preprocessing.py
```

#### ìˆ˜í–‰ ì‘ì—…
1. **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**
   - ìˆ˜ì¹˜í˜•: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
   - ë²”ì£¼í˜•: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´

2. **ë‚ ì§œ íŠ¹ì„± ì¶”ì¶œ**
   - year, month, day, dayofweek, quarter

3. **ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©**
   - Label Encoding
   - ì¹´í…Œê³ ë¦¬ ìˆ˜ê°€ ë§ì€ ê²½ìš° ìƒìœ„ 50ê°œë§Œ ìœ ì§€

4. **ì¶”ê°€ íŠ¹ì„± ìƒì„±**
   - í‰ê·  ê°€ê²© ë“± íŒŒìƒ ë³€ìˆ˜

5. **ì´ìƒì¹˜ ì œê±°**
   - IQR ë°©ë²• (threshold=1.5)

6. **íŠ¹ì„± ì •ê·œí™”**
   - StandardScaler ì ìš©

7. **ë°ì´í„° ë¶„í• **
   - Train: 70%
   - Validation: 10%
   - Test: 20%

#### ìƒì„±ë˜ëŠ” íŒŒì¼
```
ğŸ’¾ X_train.npy          # í•™ìŠµ íŠ¹ì„± ë°ì´í„°
ğŸ’¾ y_train.npy          # í•™ìŠµ íƒ€ê²Ÿ ë°ì´í„°
ğŸ’¾ X_val.npy            # ê²€ì¦ íŠ¹ì„± ë°ì´í„°
ğŸ’¾ y_val.npy            # ê²€ì¦ íƒ€ê²Ÿ ë°ì´í„°
ğŸ’¾ X_test.npy           # í…ŒìŠ¤íŠ¸ íŠ¹ì„± ë°ì´í„°
ğŸ’¾ y_test.npy           # í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°
ğŸ’¾ preprocessor.pkl     # ì „ì²˜ë¦¬ ê°ì²´ (ìŠ¤ì¼€ì¼ëŸ¬, ì¸ì½”ë”)
```

---

### Step 5: ëª¨ë¸ í•™ìŠµ

#### ê¸°ë³¸ ì‹¤í–‰
```bash
python train.py --epochs 100 --batch_size 32 --lr 0.001
```

#### ëª¨ë¸ íƒ€ì…ë³„ í•™ìŠµ

**1. Basic ëª¨ë¸ (ê¸°ë³¸ MLP)**
```bash
python train.py \
  --model_type basic \
  --epochs 100 \
  --batch_size 32 \
  --lr 0.001
```
- 3ê°œì˜ ì™„ì „ ì—°ê²°ì¸µ (256â†’128â†’64)
- BatchNorm + ReLU + Dropout

**2. Advanced ëª¨ë¸ (ì”ì°¨ ì—°ê²°)**
```bash
python train.py \
  --model_type advanced \
  --epochs 100 \
  --batch_size 32 \
  --lr 0.001
```
- Residual Blocks
- Skip connections
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ìš©ì´

**3. Attention ëª¨ë¸**
```bash
python train.py \
  --model_type attention \
  --epochs 100 \
  --batch_size 32 \
  --lr 0.001
```
- Multi-head Self-Attention
- Feed-forward Network
- íŠ¹ì„± ê°„ ê´€ê³„ í•™ìŠµ

#### ì»¤ìŠ¤í…€ í•˜ì´í¼íŒŒë¼ë¯¸í„°
```bash
python train.py \
  --model_type basic \
  --hidden_dims 512 256 128 \
  --dropout 0.4 \
  --epochs 150 \
  --batch_size 64 \
  --lr 0.0005 \
  --weight_decay 1e-4 \
  --patience 20
```

#### ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¸°ë³¸ê°’ |
|---------|------|--------|
| `--model_type` | ëª¨ë¸ íƒ€ì… (basic/advanced/attention) | basic |
| `--hidden_dims` | ì€ë‹‰ì¸µ ì°¨ì› ë¦¬ìŠ¤íŠ¸ | [256, 128, 64] |
| `--dropout` | ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ | 0.3 |
| `--epochs` | í•™ìŠµ ì—í¬í¬ ìˆ˜ | 100 |
| `--batch_size` | ë°°ì¹˜ í¬ê¸° | 32 |
| `--lr` | í•™ìŠµë¥  | 0.001 |
| `--weight_decay` | L2 ì •ê·œí™” ê°€ì¤‘ì¹˜ | 1e-5 |
| `--patience` | Early Stopping patience | 15 |
| `--seed` | ëœë¤ ì‹œë“œ | 42 |

#### ìƒì„±ë˜ëŠ” íŒŒì¼
```
ğŸ“ models/
  â”œâ”€â”€ best_model.pth           # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
  â”œâ”€â”€ config.json              # ëª¨ë¸ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
  â””â”€â”€ training_history.png     # í•™ìŠµ/ê²€ì¦ ì†ì‹¤ ê³¡ì„ 
```

#### í•™ìŠµ ê³¼ì •ì—ì„œ ì œê³µë˜ëŠ” ê¸°ëŠ¥
- âœ… **Early Stopping**: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¢…ë£Œ
- âœ… **Learning Rate Scheduling**: ReduceLROnPlateauë¡œ í•™ìŠµë¥  ìë™ ì¡°ì •
- âœ… **ëª¨ë¸ ì²´í¬í¬ì¸íŒ…**: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì €ì¥
- âœ… **Progress Bar**: ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™© í‘œì‹œ
- âœ… **GPU ì§€ì›**: CUDA ìë™ ê°ì§€ ë° ì‚¬ìš©

---

### Step 6: ì˜ˆì¸¡ ë° í‰ê°€

#### ê¸°ë³¸ ì‹¤í–‰
```bash
python predict.py --model_path models/best_model.pth
```

#### ìƒì„¸ ì˜µì…˜
```bash
python predict.py \
  --model_path models/best_model.pth \
  --config_path models/config.json \
  --test_data X_test.npy \
  --test_labels y_test.npy \
  --batch_size 32 \
  --use_dataloader \
  --save_dir results
```

#### ìˆ˜í–‰ ì‘ì—…
1. **ëª¨ë¸ ë¡œë“œ**
   - ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
   - ëª¨ë¸ ì„¤ì • ë³µì›

2. **ì˜ˆì¸¡ ìˆ˜í–‰**
   - í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
   - ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì  ê³„ì‚°

3. **ì„±ëŠ¥ í‰ê°€**
   - MAE (Mean Absolute Error)
   - MSE (Mean Squared Error)
   - RMSE (Root Mean Squared Error)
   - RÂ² Score

4. **ê²°ê³¼ ì‹œê°í™”**
   - Actual vs Predicted scatter plot
   - Residual plot
   - Residual distribution
   - Time series comparison

#### ìƒì„±ë˜ëŠ” íŒŒì¼
```
ğŸ“ results/
  â”œâ”€â”€ evaluation_results.json         # ì„±ëŠ¥ ì§€í‘œ JSON
  â”œâ”€â”€ predictions.csv                 # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸
  â””â”€â”€ prediction_visualization.png    # 4ê°€ì§€ ì‹œê°í™” í”Œë¡¯
```

#### predictions.csv êµ¬ì¡°
```csv
Actual,Predicted,Residual,Absolute_Error
100.5,98.3,2.2,2.2
200.1,205.7,-5.6,5.6
...
```

---

## ğŸ“Š ëª¨ë¸ ì•„í‚¤í…ì²˜

### 1. Basic Model (SalesPredictor)
```
Input (n_features)
    â†“
Linear(n_features â†’ 256) + BatchNorm + ReLU + Dropout(0.3)
    â†“
Linear(256 â†’ 128) + BatchNorm + ReLU + Dropout(0.3)
    â†“
Linear(128 â†’ 64) + BatchNorm + ReLU + Dropout(0.3)
    â†“
Linear(64 â†’ 1)
    â†“
Output (prediction)
```

**íŠ¹ì§•:**
- ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì 
- ëŒ€ë¶€ë¶„ì˜ íšŒê·€ ë¬¸ì œì— ì í•©
- ë¹ ë¥¸ í•™ìŠµ ì†ë„

### 2. Advanced Model (AdvancedSalesPredictor)
```
Input (n_features)
    â†“
Linear(n_features â†’ 256) + BatchNorm + ReLU + Dropout
    â†“
Residual Block 1 (256 â†’ 256)
    â†“
Residual Block 2 (256 â†’ 256)
    â†“
Residual Block 3 (256 â†’ 256)
    â†“
Linear(256 â†’ 128) + BatchNorm + ReLU + Dropout
    â†“
Linear(128 â†’ 1)
    â†“
Output (prediction)
```

**íŠ¹ì§•:**
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
- Gradient vanishing ë¬¸ì œ ì™„í™”
- ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ

### 3. Attention Model (AttentionSalesPredictor)
```
Input (n_features)
    â†“
Feature Embedding (n_features â†’ 256)
    â†“
Multi-head Self-Attention (4 heads)
    â†“
Residual + Layer Norm
    â†“
Feed-Forward Network
    â†“
Residual + Layer Norm
    â†“
Output Layer
    â†“
Output (prediction)
```

**íŠ¹ì§•:**
- íŠ¹ì„± ê°„ ê´€ê³„ í•™ìŠµ
- ì¤‘ìš”í•œ íŠ¹ì„±ì— ì§‘ì¤‘
- í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ

---

## ğŸ’¡ ì‚¬ìš© íŒ

### 1. GPU ì‚¬ìš©
CUDAê°€ ì„¤ì¹˜ëœ í™˜ê²½ì—ì„œëŠ” ìë™ìœ¼ë¡œ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```python
# í•™ìŠµ ì‹œ ìë™ìœ¼ë¡œ ê°ì§€
Using device: cuda
GPU 0: NVIDIA GeForce RTX 3080
```

### 2. ëª¨ë¸ ë¹„êµ
ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ ë¹„êµ:
```bash
# Basic ëª¨ë¸
python train.py --model_type basic --save_dir models/basic

# Advanced ëª¨ë¸
python train.py --model_type advanced --save_dir models/advanced

# Attention ëª¨ë¸
python train.py --model_type attention --save_dir models/attention

# ê° ëª¨ë¸ í‰ê°€
python predict.py --model_path models/basic/best_model.pth --save_dir results/basic
python predict.py --model_path models/advanced/best_model.pth --save_dir results/advanced
python predict.py --model_path models/attention/best_model.pth --save_dir results/attention
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°:
- **Learning Rate**: [0.0001, 0.0005, 0.001, 0.005]
- **Batch Size**: [16, 32, 64, 128]
- **Hidden Dimensions**: [128,64], [256,128,64], [512,256,128]
- **Dropout**: [0.2, 0.3, 0.4, 0.5]

### 4. Early Stopping
ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¢…ë£Œ:
```bash
# patienceë¥¼ ëŠ˜ë¦¬ë©´ ë” ì˜¤ë˜ í•™ìŠµ
python train.py --patience 20

# patienceë¥¼ ì¤„ì´ë©´ ë¹ ë¥´ê²Œ ì¢…ë£Œ
python train.py --patience 5
```

### 5. í•™ìŠµ ì¬ê°œ
ì¤‘ë‹¨ëœ í•™ìŠµì„ ì¬ê°œí•˜ë ¤ë©´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€ í•„ìš” (í˜„ì¬ëŠ” ì²˜ìŒë¶€í„° í•™ìŠµ)

---

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘ (ì „ì²´ íŒŒì´í”„ë¼ì¸)

ì²˜ìŒ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” ê²½ìš°:

```bash
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 2. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Kaggleì—ì„œ)
# Amazon Sale Report.csvë¥¼ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥

# 3. ë°ì´í„° ë¶„ì„
python data_analysis.py

# 4. ë°ì´í„° ì „ì²˜ë¦¬
python data_preprocessing.py

# 5. ëª¨ë¸ í•™ìŠµ
python train.py --epochs 100

# 6. ì˜ˆì¸¡ ë° í‰ê°€
python predict.py
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- ë°ì´í„° ë¶„ì„: 2-5ë¶„
- ë°ì´í„° ì „ì²˜ë¦¬: 5-10ë¶„
- ëª¨ë¸ í•™ìŠµ: 10-30ë¶„ (ë°ì´í„° í¬ê¸° ë° í•˜ë“œì›¨ì–´ì— ë”°ë¼)
- ì˜ˆì¸¡ ë° í‰ê°€: 1-2ë¶„

---

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ í•´ì„

### MAE (Mean Absolute Error)
- ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
- **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ**
- í•´ì„: í‰ê· ì ìœ¼ë¡œ ì˜ˆì¸¡ì´ ì‹¤ì œê°’ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ì§€

### RMSE (Root Mean Squared Error)
- ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨
- **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ**
- MAEë³´ë‹¤ í° ì˜¤ì°¨ì— ë” ë¯¼ê°

### RÂ² Score
- ëª¨ë¸ì˜ ì„¤ëª…ë ¥
- **1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ**
- 0.8 ì´ìƒ: ë§¤ìš° ì¢‹ìŒ
- 0.6-0.8: ì¢‹ìŒ
- 0.4-0.6: ë³´í†µ
- 0.4 ë¯¸ë§Œ: ê°œì„  í•„ìš”

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### 1. CUDA Out of Memory
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python train.py --batch_size 16

# ë˜ëŠ” ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
python train.py --hidden_dims 128 64
```

### 2. í•™ìŠµì´ ëŠë¦° ê²½ìš°
```bash
# ë°°ì¹˜ í¬ê¸° ëŠ˜ë¦¬ê¸° (GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œ ê²½ìš°)
python train.py --batch_size 128

# ì—í¬í¬ ìˆ˜ ì¤„ì´ê¸°
python train.py --epochs 50
```

### 3. Overfitting ë°œìƒ
```bash
# Dropout ì¦ê°€
python train.py --dropout 0.5

# Weight decay ì¦ê°€
python train.py --weight_decay 1e-4

# Early stopping patience ì¤„ì´ê¸°
python train.py --patience 10
```

### 4. Underfitting ë°œìƒ
```bash
# ëª¨ë¸ í¬ê¸° ì¦ê°€
python train.py --hidden_dims 512 256 128 64

# Dropout ê°ì†Œ
python train.py --dropout 0.2

# í•™ìŠµë¥  ì¡°ì •
python train.py --lr 0.0005
```

---

## ğŸ“ í•™ìŠµ ìë£Œ

PyTorch ë° ë”¥ëŸ¬ë‹ í•™ìŠµì„ ìœ„í•œ ì¶”ì²œ ìë£Œ:
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/index.html)
- [PyTorch íŠœí† ë¦¬ì–¼](https://pytorch.org/tutorials/)
- [Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning)
- [Fast.ai](https://www.fast.ai/)

---
